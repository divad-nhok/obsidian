{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of geophysical sensor data to inform priors\n",
    "\n",
    "Since we don't have a really great idea of what constitutes a good set of priors for real data, here I try my best to sort out what is going on using what I hope will be simple, but robust, assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise and length scale characteristics for gravity and magnetism\n",
    "\n",
    "We've been running with some set of priors for gravity and magnetism, but in all fairness we have no idea what those should be.  We know they're both linear sensors that integrate over rock properties, with a 3-D sensitivity profile that gets broader with depth.  So by fitting a GP to them, we get some idea of the noise, and a lower limit on the relevant length scale.  Since they're on a grid, we could also consider the autocorrelation.\n",
    "\n",
    "This isn't really meant to be a Bayesian analysis, but it's meant to give us some idea of the order of magnitude of the noise in a model that's flexible enough to respond to changes, but that insists on smoothness so we can pick off the delta-function component of the covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '/Users/davidkohn/dev/obsidian/data'\n",
    "fname_grav = 'gravity_400m_Gascoyne.txt'\n",
    "fname_mag = 'mag_TMI_gascoyne.txt'\n",
    "lt_val_grav = 0.05\n",
    "lt_val_mag = 0.015\n",
    "list_fnames = [\n",
    "    fname_grav,\n",
    "    fname_mag\n",
    "]\n",
    "list_less_than_values = [\n",
    "    0.05,\n",
    "    0.015\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_str0 = 'fpath: {}'\n",
    "msg_str1 = '  Latitude min: {}\\n  Latitude max: {}'\n",
    "msg_str2 = '  Grid code min: {}\\n  Grid code max: {}'\n",
    "msg_str3 = '  Data shape: {}'\n",
    "msg_str4 = '  X shape: {}\\n  Y shape: {}'\n",
    "\n",
    "def get_data_stats(fpath, lt_val):\n",
    "    msg_str = msg_str0.format(fpath)\n",
    "    print(msg_str)\n",
    "    data = pd.read_csv(fpath, header=0)\n",
    "    msg_str = msg_str1.format(data.Latitude.min(), gravdata.Latitude.max())\n",
    "    print(msg_str)\n",
    "    # q1. why add/subtract these values from lat and lon?\n",
    "    # q2. why only take data less than a certain value?\n",
    "    data = data[np.abs(data.Latitude + 24.85) < lt_val]\n",
    "    data = data[np.abs(data.Longitude - 116.1) < lt_val]\n",
    "    msg_str = msg_str2.format(data.Latitude.min(), gravdata.Latitude.max())\n",
    "    print(msg_str)\n",
    "    msg_str = msg_str3.format(data.shape)\n",
    "    print(msg_str)\n",
    "    return(data)\n",
    "\n",
    "def run_gp(data):\n",
    "    X = np.array([data.Latitude, data.Longitude]).T\n",
    "    Y = np.array([data.grid_code]).T\n",
    "    kernel = GPy.kern.Matern32(2)\n",
    "    msg_str = msg_str4.format(X.shape, Y.shape)\n",
    "    model = GPy.models.GPRegression(X, Y, kernel)\n",
    "    model.optimize(messages=True)\n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "    f = model.plot()\n",
    "    print(model)\n",
    "    return(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grav\n",
    "This seems pretty weird -- the gravity data seems to have a very long length scale and no obvious noise.  But we can see from the contours that there is some structure.  Not sure what to make of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = os.path.join(dir_data, fname_grav)\n",
    "data = get_data_stats(fpath, lt_val_grav)\n",
    "X, Y = run_gp(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mag\n",
    "Magnetism, on the other hand, has at least some non-zero Gaussian noise to it.  But surely the length scale is kind of out of whack?  And are those repeated points there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = os.path.join(dir_data, fname_mag)\n",
    "data = get_data_stats(fpath, lt_val_mag)\n",
    "X, Y = run_gp(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooh looks like they are.  Well, in a way that's useful, if those are real -- in principle they give us the noise scale.  But if it isn't real, it's not clear this would have worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at mag X, Y\n",
    "dX0 = X[:,0].reshape(36,36)[:,0]\n",
    "print(dX0)\n",
    "print(dX0[1:] - dX0[:-1])\n",
    "dX1 = X[:,1].reshape(36,36)[1,:]\n",
    "print(dX1)\n",
    "print(dX1[1:] - dX1[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ipy361",
   "language": "python",
   "name": "ipy361"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
